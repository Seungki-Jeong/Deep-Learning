{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, model_selection\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('data/fashion-mnist/train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    mnist_data = np.frombuffer(f.read(), np.uint8, offset = 16)\n",
    "    mnist_data = mnist_data.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.298039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4         5    6    7         8         9    ...  \\\n",
       "0      0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.000000  ...   \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.003922  0.0  0.0  0.000000  0.000000  ...   \n",
       "2      0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.086275  ...   \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.129412  0.376471  ...   \n",
       "4      0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.000000  ...   \n",
       "...    ...  ...  ...  ...  ...       ...  ...  ...       ...       ...  ...   \n",
       "59995  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.000000  ...   \n",
       "59996  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.000000  ...   \n",
       "59997  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.019608  ...   \n",
       "59998  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.000000  ...   \n",
       "59999  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.000000  ...   \n",
       "\n",
       "            774       775       776       777  778       779  780  781  782  \\\n",
       "0      0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.0   \n",
       "1      0.466667  0.447059  0.509804  0.298039  0.0  0.000000  0.0  0.0  0.0   \n",
       "2      0.000000  0.000000  0.003922  0.000000  0.0  0.000000  0.0  0.0  0.0   \n",
       "3      0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.0   \n",
       "4      0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.0   \n",
       "...         ...       ...       ...       ...  ...       ...  ...  ...  ...   \n",
       "59995  0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.0   \n",
       "59996  0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.0   \n",
       "59997  0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.0   \n",
       "59998  0.258824  0.211765  0.196078  0.019608  0.0  0.003922  0.0  0.0  0.0   \n",
       "59999  0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.0   \n",
       "\n",
       "       783  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "...    ...  \n",
       "59995  0.0  \n",
       "59996  0.0  \n",
       "59997  0.0  \n",
       "59998  0.0  \n",
       "59999  0.0  \n",
       "\n",
       "[60000 rows x 784 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규화\n",
    "mnist_data = mnist_data / 255\n",
    "\n",
    "pd.DataFrame(mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist_data[0].reshape(28, 28), cmap = cm.gray_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('data/fashion-mnist/train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    mnist_label = np.frombuffer(f.read(), np.uint8, offset = 8)\n",
    "    \n",
    "mnist_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 0, ..., 3, 0, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_label = np.array(mnist_label).astype(int)\n",
    "mnist_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(mnist_data))\n",
    "print(type(mnist_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 5000\n",
    "test_size = 500\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = model_selection.train_test_split(\n",
    "    mnist_data, mnist_label, train_size = train_size, test_size = test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.reshape((len(train_X), 1, 28, 28))\n",
    "test_X = test_X.reshape((len(test_X), 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 텐서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 1, 28, 28])\n",
      "torch.Size([5000])\n"
     ]
    }
   ],
   "source": [
    "train_X = torch.from_numpy(train_X).float()\n",
    "train_Y = torch.from_numpy(train_Y).long()\n",
    "\n",
    "test_X = torch.from_numpy(test_X).float()\n",
    "test_Y = torch.from_numpy(test_Y).long()\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.3020, 0.3176, 0.3098, 0.2863,\n",
      "          0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.6471, 0.7608, 0.5020, 0.5647, 0.7922,\n",
      "          0.4353, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0549, 0.0314, 0.4706, 0.9176, 0.7765, 0.8784, 0.9098,\n",
      "          0.2824, 0.0824, 0.0471, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0745,\n",
      "          0.1843, 0.2471, 0.1882, 0.0353, 0.8118, 0.5843, 0.7373, 0.5843,\n",
      "          0.0157, 0.2157, 0.2588, 0.2000, 0.0941, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0392, 0.2157, 0.2471,\n",
      "          0.1882, 0.1725, 0.1569, 0.1412, 0.1569, 0.1020, 0.1647, 0.1451,\n",
      "          0.1412, 0.1725, 0.1725, 0.2039, 0.2549, 0.2314, 0.0275, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3059, 0.2980, 0.1176,\n",
      "          0.2235, 0.1961, 0.1765, 0.1804, 0.1647, 0.2235, 0.1451, 0.1686,\n",
      "          0.1882, 0.1765, 0.1686, 0.1765, 0.1647, 0.2431, 0.2431, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.3569, 0.3137, 0.1765,\n",
      "          0.1569, 0.2275, 0.1882, 0.1804, 0.1804, 0.2078, 0.1882, 0.1765,\n",
      "          0.1765, 0.1843, 0.1843, 0.1843, 0.1961, 0.2078, 0.2902, 0.0510,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.1294, 0.2902, 0.2941, 0.2941,\n",
      "          0.1098, 0.1843, 0.2118, 0.1843, 0.1882, 0.2039, 0.1882, 0.1922,\n",
      "          0.1922, 0.1843, 0.2000, 0.1922, 0.2196, 0.1961, 0.2471, 0.1765,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2588, 0.2118, 0.2431, 0.4627,\n",
      "          0.1529, 0.1647, 0.1922, 0.1725, 0.1882, 0.2000, 0.1804, 0.1922,\n",
      "          0.2039, 0.1843, 0.2078, 0.1843, 0.2000, 0.2431, 0.2196, 0.2706,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0902, 0.0902, 0.1961, 0.5686,\n",
      "          0.1843, 0.1922, 0.1922, 0.1843, 0.1882, 0.1961, 0.1843, 0.1922,\n",
      "          0.1882, 0.1765, 0.1765, 0.1804, 0.2118, 0.2706, 0.0627, 0.0706,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.7333, 0.8275, 0.2510, 0.0000, 0.5059,\n",
      "          0.2588, 0.1490, 0.2118, 0.1882, 0.1882, 0.1882, 0.1922, 0.2000,\n",
      "          0.1882, 0.1765, 0.1765, 0.1686, 0.1529, 0.1490, 0.2627, 0.8353,\n",
      "          0.7216, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0980, 0.5804, 1.0000, 0.7333, 0.8039,\n",
      "          0.2157, 0.2078, 0.1961, 0.2000, 0.2039, 0.2039, 0.1843, 0.1922,\n",
      "          0.2000, 0.1922, 0.2235, 0.1216, 0.5765, 0.8196, 0.7020, 0.5059,\n",
      "          0.0157, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2627, 0.2745,\n",
      "          0.2431, 0.2353, 0.1882, 0.1843, 0.1961, 0.2275, 0.1804, 0.1922,\n",
      "          0.1922, 0.2000, 0.1922, 0.1529, 0.2000, 0.2118, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0039, 0.0078, 0.0000, 0.0000, 0.0000,\n",
      "          0.3961, 0.2039, 0.1882, 0.1882, 0.1922, 0.2275, 0.1882, 0.1882,\n",
      "          0.1686, 0.1922, 0.1647, 0.2510, 0.0000, 0.0000, 0.0000, 0.0078,\n",
      "          0.0039, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.0235,\n",
      "          0.4000, 0.1882, 0.1961, 0.2039, 0.1961, 0.2078, 0.1882, 0.1882,\n",
      "          0.1647, 0.1882, 0.1686, 0.2627, 0.0314, 0.0000, 0.0039, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0275,\n",
      "          0.4078, 0.1843, 0.1961, 0.2078, 0.1961, 0.1961, 0.1843, 0.1922,\n",
      "          0.1725, 0.1843, 0.1725, 0.2706, 0.0314, 0.0000, 0.0039, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0392,\n",
      "          0.4078, 0.1725, 0.2157, 0.1961, 0.1765, 0.2196, 0.1922, 0.1843,\n",
      "          0.1765, 0.1882, 0.1765, 0.2784, 0.0275, 0.0000, 0.0039, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0549,\n",
      "          0.4196, 0.1529, 0.2353, 0.2000, 0.1804, 0.2118, 0.1922, 0.1922,\n",
      "          0.1686, 0.1922, 0.1804, 0.2863, 0.0471, 0.0000, 0.0039, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0824,\n",
      "          0.4235, 0.1490, 0.2471, 0.2000, 0.1765, 0.2078, 0.1961, 0.1961,\n",
      "          0.1608, 0.1843, 0.1725, 0.2863, 0.0588, 0.0000, 0.0039, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.1333,\n",
      "          0.4118, 0.1529, 0.2706, 0.1804, 0.1882, 0.2157, 0.1765, 0.2039,\n",
      "          0.1569, 0.1843, 0.1647, 0.2745, 0.0667, 0.0000, 0.0039, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.1647,\n",
      "          0.4157, 0.1569, 0.2941, 0.1725, 0.1922, 0.2235, 0.2000, 0.1882,\n",
      "          0.1647, 0.1922, 0.1647, 0.2667, 0.0863, 0.0000, 0.0039, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.1843,\n",
      "          0.4078, 0.1804, 0.2784, 0.1765, 0.1922, 0.1843, 0.2196, 0.2275,\n",
      "          0.1686, 0.1843, 0.1647, 0.2549, 0.1137, 0.0000, 0.0039, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2510,\n",
      "          0.3961, 0.2275, 0.2431, 0.1686, 0.1961, 0.2157, 0.1765, 0.2431,\n",
      "          0.1686, 0.1804, 0.1686, 0.2431, 0.1333, 0.0000, 0.0039, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2941,\n",
      "          0.3843, 0.2784, 0.1882, 0.2039, 0.1961, 0.2235, 0.1765, 0.2275,\n",
      "          0.2000, 0.1765, 0.1725, 0.2235, 0.1725, 0.0000, 0.0039, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3765,\n",
      "          0.4078, 0.2745, 0.1765, 0.2000, 0.1922, 0.2510, 0.1451, 0.2314,\n",
      "          0.2157, 0.1686, 0.1804, 0.2118, 0.2353, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2510,\n",
      "          0.4314, 0.2157, 0.1961, 0.2000, 0.1961, 0.3216, 0.1529, 0.2118,\n",
      "          0.2235, 0.1686, 0.1608, 0.2431, 0.2039, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.3922, 0.2824, 0.2353, 0.2235, 0.2078, 0.3176, 0.1451, 0.1922,\n",
      "          0.2549, 0.1922, 0.2235, 0.2549, 0.0157, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0078, 0.1490, 0.1961, 0.1961, 0.1686, 0.2980, 0.1765, 0.1529,\n",
      "          0.2314, 0.1765, 0.1255, 0.0235, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]]), tensor(6))\n"
     ]
    }
   ],
   "source": [
    "train = TensorDataset(train_X, train_Y)\n",
    "\n",
    "print(train[0])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "#         합성곱층\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) # 입력 채널 수, 출력 채널 수, 필터 크기\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "#         전결합층\n",
    "        self.fc1 = nn.Linear(256, 64) # 256 = (((28 - 5 + 1) / 2) -5 + 1) / 2 * (((28 - 5 + 1) / 2) - 5 + 1) / 2 * 16\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2) # 풀링 영역 크기\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 256)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14\\.conda\\envs\\pt\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tensor(22.7138)\n",
      "200 tensor(16.6765)\n",
      "300 tensor(12.1052)\n",
      "400 tensor(8.7162)\n",
      "500 tensor(5.6728)\n",
      "600 tensor(3.3725)\n",
      "700 tensor(2.4667)\n",
      "800 tensor(1.1868)\n",
      "900 tensor(0.6662)\n",
      "1000 tensor(0.2341)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for train_x, train_y in train_loader:\n",
    "        train_x, train_y = Variable(train_x), Variable(train_y)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = criterion(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.data\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(epoch+1, total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14\\.conda\\envs\\pt\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.862"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x, test_y = Variable(test_X), Variable(test_Y)\n",
    "\n",
    "result = torch.max(model(test_x).data, 1)[1]\n",
    "\n",
    "accuracy = sum(test_y.data.numpy() == result.numpy()) / len(test_y.data.numpy())\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
