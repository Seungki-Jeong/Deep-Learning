{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더 지정\n",
    "dirs = ['ants', 'bees']\n",
    "\n",
    "data = []\n",
    "label = []\n",
    "\n",
    "# error_cnt = 1\n",
    "for i, d in enumerate(dirs):\n",
    "    files = os.listdir('data/hymenoptera_data/' + d)\n",
    "    \n",
    "    for f in files:\n",
    "        img = Image.open('data/hymenoptera_data/' + d + '/' + f, 'r')\n",
    "#         print(error_cnt)\n",
    "#         error_cnt += 1\n",
    "#         print(f)\n",
    "        resize_img = img.resize((128, 128))\n",
    "        \n",
    "        r, g, b = resize_img.split()\n",
    "        r_resize_img = np.asarray(np.float32(r) / 255.0)\n",
    "        g_resize_img = np.asarray(np.float32(g) / 255.0)\n",
    "        b_resize_img = np.asarray(np.float32(b) / 255.0)\n",
    "        rgb_resize_img = np.asarray([r_resize_img, g_resize_img, b_resize_img])\n",
    "        \n",
    "        data.append(rgb_resize_img)\n",
    "        \n",
    "        label.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313726</td>\n",
       "      <td>0.313726</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.313726</td>\n",
       "      <td>0.313726</td>\n",
       "      <td>0.313726</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.305882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313726</td>\n",
       "      <td>0.313726</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.313726</td>\n",
       "      <td>0.313726</td>\n",
       "      <td>0.301961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.305882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.305882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.313726</td>\n",
       "      <td>0.313726</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.298039</td>\n",
       "      <td>0.298039</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.305882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.341176</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.341176</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.564706</td>\n",
       "      <td>0.380392</td>\n",
       "      <td>0.556863</td>\n",
       "      <td>0.419608</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.537255</td>\n",
       "      <td>0.462745</td>\n",
       "      <td>0.545098</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.117647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.341176</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.443137</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.368627</td>\n",
       "      <td>0.384314</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>0.231373</td>\n",
       "      <td>0.556863</td>\n",
       "      <td>0.125490</td>\n",
       "      <td>0.105882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.341176</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>0.341176</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.521569</td>\n",
       "      <td>0.380392</td>\n",
       "      <td>0.596078</td>\n",
       "      <td>0.423529</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.105882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.341176</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219608</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.113725</td>\n",
       "      <td>0.105882</td>\n",
       "      <td>0.180392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.341176</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.341176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.149020</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.149020</td>\n",
       "      <td>0.145098</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.227451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0    0.317647  0.317647  0.317647  0.317647  0.317647  0.329412  0.329412   \n",
       "1    0.317647  0.317647  0.321569  0.321569  0.321569  0.321569  0.321569   \n",
       "2    0.321569  0.317647  0.317647  0.317647  0.321569  0.321569  0.321569   \n",
       "3    0.317647  0.317647  0.325490  0.325490  0.325490  0.329412  0.325490   \n",
       "4    0.325490  0.321569  0.325490  0.329412  0.325490  0.329412  0.325490   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "123  0.341176  0.333333  0.333333  0.337255  0.341176  0.337255  0.337255   \n",
       "124  0.337255  0.333333  0.337255  0.341176  0.333333  0.333333  0.333333   \n",
       "125  0.337255  0.337255  0.341176  0.337255  0.329412  0.341176  0.337255   \n",
       "126  0.337255  0.337255  0.337255  0.333333  0.337255  0.341176  0.337255   \n",
       "127  0.337255  0.337255  0.337255  0.341176  0.337255  0.337255  0.337255   \n",
       "\n",
       "          7         8         9    ...       118       119       120  \\\n",
       "0    0.321569  0.321569  0.321569  ...  0.313726  0.313726  0.301961   \n",
       "1    0.321569  0.329412  0.321569  ...  0.313726  0.313726  0.309804   \n",
       "2    0.325490  0.321569  0.325490  ...  0.309804  0.305882  0.317647   \n",
       "3    0.329412  0.325490  0.325490  ...  0.309804  0.309804  0.309804   \n",
       "4    0.325490  0.325490  0.325490  ...  0.317647  0.313726  0.313726   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "123  0.333333  0.329412  0.329412  ...  0.564706  0.380392  0.556863   \n",
       "124  0.333333  0.329412  0.329412  ...  0.443137  0.470588  0.368627   \n",
       "125  0.333333  0.337255  0.333333  ...  0.521569  0.380392  0.596078   \n",
       "126  0.337255  0.333333  0.329412  ...  0.219608  0.015686  0.141176   \n",
       "127  0.337255  0.337255  0.341176  ...  0.133333  0.156863  0.141176   \n",
       "\n",
       "          121       122       123       124       125       126       127  \n",
       "0    0.305882  0.305882  0.313726  0.313726  0.313726  0.309804  0.305882  \n",
       "1    0.309804  0.309804  0.309804  0.305882  0.313726  0.313726  0.301961  \n",
       "2    0.309804  0.301961  0.301961  0.305882  0.305882  0.301961  0.305882  \n",
       "3    0.309804  0.305882  0.305882  0.301961  0.301961  0.305882  0.305882  \n",
       "4    0.305882  0.301961  0.298039  0.298039  0.305882  0.309804  0.305882  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "123  0.419608  0.337255  0.537255  0.462745  0.545098  0.011765  0.117647  \n",
       "124  0.384314  0.129412  0.576471  0.231373  0.556863  0.125490  0.105882  \n",
       "125  0.423529  0.364706  0.541176  0.329412  0.568627  0.129412  0.105882  \n",
       "126  0.058824  0.176471  0.113725  0.105882  0.180392  0.000000  0.125490  \n",
       "127  0.149020  0.137255  0.149020  0.145098  0.141176  0.156863  0.227451  \n",
       "\n",
       "[128 rows x 128 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data, dtype = 'float32')\n",
    "label = np.array(label, dtype = 'int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_Y, test_Y = model_selection.train_test_split(\n",
    "data, label, test_size = 0.1)\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([357, 3, 128, 128])\n",
      "torch.Size([357])\n"
     ]
    }
   ],
   "source": [
    "train_X = torch.from_numpy(train_X).float()\n",
    "train_Y = torch.from_numpy(train_Y).long()\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.1569, 0.1451, 0.1176,  ..., 0.0549, 0.0824, 0.1451],\n",
      "         [0.1451, 0.1294, 0.1059,  ..., 0.0549, 0.0824, 0.1255],\n",
      "         [0.1451, 0.1216, 0.0980,  ..., 0.0588, 0.0667, 0.1098],\n",
      "         ...,\n",
      "         [0.1490, 0.1373, 0.1451,  ..., 0.3216, 0.3176, 0.3059],\n",
      "         [0.2118, 0.1804, 0.1765,  ..., 0.3412, 0.3216, 0.3176],\n",
      "         [0.2745, 0.2118, 0.2078,  ..., 0.3451, 0.3373, 0.3255]],\n",
      "\n",
      "        [[0.2431, 0.2118, 0.1647,  ..., 0.1294, 0.1529, 0.2157],\n",
      "         [0.2510, 0.2039, 0.1529,  ..., 0.1137, 0.1451, 0.2118],\n",
      "         [0.2549, 0.1922, 0.1373,  ..., 0.1255, 0.1451, 0.1961],\n",
      "         ...,\n",
      "         [0.1882, 0.1608, 0.1647,  ..., 0.3294, 0.3294, 0.3137],\n",
      "         [0.2314, 0.1961, 0.1961,  ..., 0.3412, 0.3294, 0.3216],\n",
      "         [0.2824, 0.2196, 0.2353,  ..., 0.3373, 0.3412, 0.3176]],\n",
      "\n",
      "        [[0.3804, 0.3137, 0.2431,  ..., 0.0510, 0.0549, 0.1255],\n",
      "         [0.4078, 0.3255, 0.2157,  ..., 0.0510, 0.0549, 0.1176],\n",
      "         [0.3961, 0.2980, 0.1765,  ..., 0.0510, 0.0431, 0.0941],\n",
      "         ...,\n",
      "         [0.1686, 0.1333, 0.1490,  ..., 0.2235, 0.2314, 0.2275],\n",
      "         [0.1765, 0.1569, 0.1804,  ..., 0.2314, 0.2353, 0.2314],\n",
      "         [0.2000, 0.1608, 0.2275,  ..., 0.2353, 0.2471, 0.2235]]]), tensor(1))\n"
     ]
    }
   ],
   "source": [
    "train = TensorDataset(train_X, train_Y)\n",
    "\n",
    "print(train[0])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 10, 5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(20 * 29 * 29, 50) # 29 = (((((128-5)+1)/2)-5)+1)/2\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 20 * 29 * 29)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모형 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14\\.conda\\envs\\pt\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 tensor(0.0049)\n",
      "100 tensor(0.0011)\n",
      "150 tensor(0.0005)\n",
      "200 tensor(0.0002)\n",
      "250 tensor(0.0001)\n",
      "300 tensor(7.9978e-05)\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "for epoch in range(300):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for train_x, train_y in train_loader:\n",
    "        train_x, train_y = Variable(train_x), Variable(train_y)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = criterion(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.data\n",
    "        \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(epoch+1, total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 3, 128, 128])\n",
      "torch.Size([40])\n"
     ]
    }
   ],
   "source": [
    "test_X = np.array(test_X, dtype = 'float32')\n",
    "test_Y = np.array(test_Y, dtype = 'int64')\n",
    "\n",
    "test_X = torch.from_numpy(test_X).float()\n",
    "test_Y = torch.from_numpy(test_Y).long()\n",
    "\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14\\.conda\\envs\\pt\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x, test_y = Variable(test_X), Variable(test_Y)\n",
    "\n",
    "result = torch.max(model(test_x).data, 1)[1]\n",
    "\n",
    "accuracy = sum(test_y.data.numpy() == result.numpy()) / len(test_y.data.numpy())\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
