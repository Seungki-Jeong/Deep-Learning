{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM을 이용한 텍스트 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleID</th>\n",
       "      <th>articleWordCount</th>\n",
       "      <th>byline</th>\n",
       "      <th>documentType</th>\n",
       "      <th>headline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>newDesk</th>\n",
       "      <th>printPage</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>snippet</th>\n",
       "      <th>source</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "      <th>webURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5adf6684068401528a2aa69b</td>\n",
       "      <td>781</td>\n",
       "      <td>By JOHN BRANCH</td>\n",
       "      <td>article</td>\n",
       "      <td>Former N.F.L. Cheerleaders’ Settlement Offer: ...</td>\n",
       "      <td>['Workplace Hazards and Violations', 'Football...</td>\n",
       "      <td>68</td>\n",
       "      <td>Sports</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 17:16:49</td>\n",
       "      <td>Pro Football</td>\n",
       "      <td>“I understand that they could meet with us, pa...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/sports/foot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5adf653f068401528a2aa697</td>\n",
       "      <td>656</td>\n",
       "      <td>By LISA FRIEDMAN</td>\n",
       "      <td>article</td>\n",
       "      <td>E.P.A. to Unveil a New Rule. Its Effect: Less ...</td>\n",
       "      <td>['Environmental Protection Agency', 'Pruitt, S...</td>\n",
       "      <td>68</td>\n",
       "      <td>Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 17:11:21</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>The agency plans to publish a new regulation T...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/climate/epa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5adf4626068401528a2aa628</td>\n",
       "      <td>2427</td>\n",
       "      <td>By PETE WELLS</td>\n",
       "      <td>article</td>\n",
       "      <td>The New Noma, Explained</td>\n",
       "      <td>['Restaurants', 'Noma (Copenhagen, Restaurant)...</td>\n",
       "      <td>66</td>\n",
       "      <td>Dining</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:58:44</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>What’s it like to eat at the second incarnatio...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/dining/noma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5adf40d2068401528a2aa619</td>\n",
       "      <td>626</td>\n",
       "      <td>By JULIE HIRSCHFELD DAVIS and PETER BAKER</td>\n",
       "      <td>article</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>['Macron, Emmanuel (1977- )', 'Trump, Donald J...</td>\n",
       "      <td>68</td>\n",
       "      <td>Washington</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:35:57</td>\n",
       "      <td>Europe</td>\n",
       "      <td>President Trump welcomed President Emmanuel Ma...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/world/europ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5adf3d64068401528a2aa60f</td>\n",
       "      <td>815</td>\n",
       "      <td>By IAN AUSTEN and DAN BILEFSKY</td>\n",
       "      <td>article</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>['Toronto, Ontario, Attack (April, 2018)', 'Mu...</td>\n",
       "      <td>68</td>\n",
       "      <td>Foreign</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:21:21</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Alek Minassian, 25, a resident of Toronto’s Ri...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/world/canad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  articleID  articleWordCount  \\\n",
       "0  5adf6684068401528a2aa69b               781   \n",
       "1  5adf653f068401528a2aa697               656   \n",
       "2  5adf4626068401528a2aa628              2427   \n",
       "3  5adf40d2068401528a2aa619               626   \n",
       "4  5adf3d64068401528a2aa60f               815   \n",
       "\n",
       "                                      byline documentType  \\\n",
       "0                             By JOHN BRANCH      article   \n",
       "1                           By LISA FRIEDMAN      article   \n",
       "2                              By PETE WELLS      article   \n",
       "3  By JULIE HIRSCHFELD DAVIS and PETER BAKER      article   \n",
       "4             By IAN AUSTEN and DAN BILEFSKY      article   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Former N.F.L. Cheerleaders’ Settlement Offer: ...   \n",
       "1  E.P.A. to Unveil a New Rule. Its Effect: Less ...   \n",
       "2                            The New Noma, Explained   \n",
       "3                                            Unknown   \n",
       "4                                            Unknown   \n",
       "\n",
       "                                            keywords  multimedia     newDesk  \\\n",
       "0  ['Workplace Hazards and Violations', 'Football...          68      Sports   \n",
       "1  ['Environmental Protection Agency', 'Pruitt, S...          68     Climate   \n",
       "2  ['Restaurants', 'Noma (Copenhagen, Restaurant)...          66      Dining   \n",
       "3  ['Macron, Emmanuel (1977- )', 'Trump, Donald J...          68  Washington   \n",
       "4  ['Toronto, Ontario, Attack (April, 2018)', 'Mu...          68     Foreign   \n",
       "\n",
       "   printPage              pubDate   sectionName  \\\n",
       "0          0  2018-04-24 17:16:49  Pro Football   \n",
       "1          0  2018-04-24 17:11:21       Unknown   \n",
       "2          0  2018-04-24 14:58:44       Unknown   \n",
       "3          0  2018-04-24 14:35:57        Europe   \n",
       "4          0  2018-04-24 14:21:21        Canada   \n",
       "\n",
       "                                             snippet              source  \\\n",
       "0  “I understand that they could meet with us, pa...  The New York Times   \n",
       "1  The agency plans to publish a new regulation T...  The New York Times   \n",
       "2  What’s it like to eat at the second incarnatio...  The New York Times   \n",
       "3  President Trump welcomed President Emmanuel Ma...  The New York Times   \n",
       "4  Alek Minassian, 25, a resident of Toronto’s Ri...  The New York Times   \n",
       "\n",
       "  typeOfMaterial                                             webURL  \n",
       "0           News  https://www.nytimes.com/2018/04/24/sports/foot...  \n",
       "1           News  https://www.nytimes.com/2018/04/24/climate/epa...  \n",
       "2           News  https://www.nytimes.com/2018/04/24/dining/noma...  \n",
       "3           News  https://www.nytimes.com/2018/04/24/world/europ...  \n",
       "4           News  https://www.nytimes.com/2018/04/24/world/canad...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('data/ArticlesApril2018.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "열의 개수:  15\n",
      "Index(['articleID', 'articleWordCount', 'byline', 'documentType', 'headline',\n",
      "       'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
      "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('열의 개수: ', len(df.columns))\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# headline 열의 데이터만 사용, Null 검사\n",
    "df['headline'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Former N.F.L. Cheerleaders’ Settlement Offer: $1 and a Meeting With Goodell',\n",
       " 'E.P.A. to Unveil a New Rule. Its Effect: Less Science in Policymaking.',\n",
       " 'The New Noma, Explained',\n",
       " 'Unknown',\n",
       " 'Unknown']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# headline 열의 데이터로 리스트 생성\n",
    "headline = [title for title in df.headline.values]\n",
    "headline[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1324"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1214"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 노이즈 데이터('Unknown') 제거\n",
    "headline = [title for title in headline if title != 'Unknown']\n",
    "len(headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구두점 제거와 소문자화를 위한 함수\n",
    "def repreprocessing(s):\n",
    "    s=s.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return ''.join(c for c in s if c not in punctuation).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['former nfl cheerleaders settlement offer 1 and a meeting with goodell',\n",
       " 'epa to unveil a new rule its effect less science in policymaking',\n",
       " 'the new noma explained',\n",
       " 'how a bag of texas dirt  became a times tradition',\n",
       " 'is school a place for selfexpression']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [repreprocessing(x) for x in headline]\n",
    "text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 3494\n"
     ]
    }
   ],
   "source": [
    "# 단어 집합(vocabulary)을 만들고 크기를 확인\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(text)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print('단어 집합의 크기 : %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'a': 2,\n",
       " 'to': 3,\n",
       " 'of': 4,\n",
       " 'in': 5,\n",
       " 'for': 6,\n",
       " 'and': 7,\n",
       " 'is': 8,\n",
       " 'on': 9,\n",
       " 'with': 10,\n",
       " 'trump': 11,\n",
       " 'as': 12,\n",
       " 'at': 13,\n",
       " 'new': 14,\n",
       " 'how': 15,\n",
       " 'from': 16,\n",
       " 'it': 17,\n",
       " 'an': 18,\n",
       " 'that': 19,\n",
       " 'be': 20,\n",
       " 'season': 21,\n",
       " 'us': 22,\n",
       " 'you': 23,\n",
       " 'its': 24,\n",
       " 'what': 25,\n",
       " 'episode': 26,\n",
       " 'can': 27,\n",
       " 'your': 28,\n",
       " 'not': 29,\n",
       " 'he': 30,\n",
       " 'now': 31,\n",
       " 'his': 32,\n",
       " 'are': 33,\n",
       " 'teaching': 34,\n",
       " 'war': 35,\n",
       " 'out': 36,\n",
       " 'no': 37,\n",
       " 'was': 38,\n",
       " 'by': 39,\n",
       " 'trumps': 40,\n",
       " 'has': 41,\n",
       " 'over': 42,\n",
       " 'may': 43,\n",
       " 'into': 44,\n",
       " 'why': 45,\n",
       " 'more': 46,\n",
       " 'we': 47,\n",
       " 'who': 48,\n",
       " 'about': 49,\n",
       " 'recap': 50,\n",
       " 'activities': 51,\n",
       " '1': 52,\n",
       " 'just': 53,\n",
       " 'do': 54,\n",
       " 'women': 55,\n",
       " 'when': 56,\n",
       " 'syria': 57,\n",
       " 'trade': 58,\n",
       " 'i': 59,\n",
       " '2': 60,\n",
       " 'or': 61,\n",
       " 'will': 62,\n",
       " 'this': 63,\n",
       " 'have': 64,\n",
       " 'president': 65,\n",
       " 'but': 66,\n",
       " 'home': 67,\n",
       " 'up': 68,\n",
       " 'long': 69,\n",
       " 'one': 70,\n",
       " 'off': 71,\n",
       " 'facebook': 72,\n",
       " 'house': 73,\n",
       " 'gop': 74,\n",
       " 'our': 75,\n",
       " 'case': 76,\n",
       " 'they': 77,\n",
       " 'life': 78,\n",
       " 'end': 79,\n",
       " 'right': 80,\n",
       " 'some': 81,\n",
       " 'big': 82,\n",
       " 'dead': 83,\n",
       " 'power': 84,\n",
       " 'say': 85,\n",
       " 'white': 86,\n",
       " 'after': 87,\n",
       " 'still': 88,\n",
       " 'north': 89,\n",
       " 'my': 90,\n",
       " 'dont': 91,\n",
       " 'need': 92,\n",
       " 'race': 93,\n",
       " 'own': 94,\n",
       " 'against': 95,\n",
       " 'here': 96,\n",
       " 'should': 97,\n",
       " 'border': 98,\n",
       " 'former': 99,\n",
       " 'epa': 100,\n",
       " 'battle': 101,\n",
       " 'mr': 102,\n",
       " 'too': 103,\n",
       " 'their': 104,\n",
       " 'plan': 105,\n",
       " '3': 106,\n",
       " 'china': 107,\n",
       " 'real': 108,\n",
       " 'were': 109,\n",
       " 'her': 110,\n",
       " 'russia': 111,\n",
       " 'art': 112,\n",
       " 'good': 113,\n",
       " 'then': 114,\n",
       " 'like': 115,\n",
       " 'pay': 116,\n",
       " 'back': 117,\n",
       " 'get': 118,\n",
       " 'love': 119,\n",
       " 'says': 120,\n",
       " 'officials': 121,\n",
       " 'fight': 122,\n",
       " 'tariffs': 123,\n",
       " 'pruitt': 124,\n",
       " 'democrats': 125,\n",
       " 'black': 126,\n",
       " 'man': 127,\n",
       " 'men': 128,\n",
       " 'help': 129,\n",
       " 'never': 130,\n",
       " 'york': 131,\n",
       " 'comey': 132,\n",
       " 'chief': 133,\n",
       " 'metoo': 134,\n",
       " 'work': 135,\n",
       " 'place': 136,\n",
       " 'could': 137,\n",
       " 'past': 138,\n",
       " 'years': 139,\n",
       " 'rights': 140,\n",
       " 'first': 141,\n",
       " 'money': 142,\n",
       " 'save': 143,\n",
       " 'going': 144,\n",
       " 'all': 145,\n",
       " 'way': 146,\n",
       " 'political': 147,\n",
       " 'fear': 148,\n",
       " 'next': 149,\n",
       " 'fire': 150,\n",
       " 'party': 151,\n",
       " 'me': 152,\n",
       " 'becomes': 153,\n",
       " '8': 154,\n",
       " 'better': 155,\n",
       " 'old': 156,\n",
       " 'dr': 157,\n",
       " 'king': 158,\n",
       " 'homes': 159,\n",
       " 'ryan': 160,\n",
       " 'tax': 161,\n",
       " 'if': 162,\n",
       " 'than': 163,\n",
       " 'americans': 164,\n",
       " 'rules': 165,\n",
       " 'police': 166,\n",
       " 'school': 167,\n",
       " 'leaders': 168,\n",
       " 'korea': 169,\n",
       " 'there': 170,\n",
       " 'top': 171,\n",
       " 'court': 172,\n",
       " 'state': 173,\n",
       " '10': 174,\n",
       " 'lower': 175,\n",
       " 'states': 176,\n",
       " 'whats': 177,\n",
       " 'use': 178,\n",
       " 'cancer': 179,\n",
       " 'britain': 180,\n",
       " 'wont': 181,\n",
       " 'time': 182,\n",
       " 'america': 183,\n",
       " 'history': 184,\n",
       " 'where': 185,\n",
       " 'lead': 186,\n",
       " 'left': 187,\n",
       " 'plans': 188,\n",
       " 'talk': 189,\n",
       " 'crisis': 190,\n",
       " 'two': 191,\n",
       " 'tells': 192,\n",
       " 'trust': 193,\n",
       " 'nuclear': 194,\n",
       " 'children': 195,\n",
       " 'million': 196,\n",
       " 'make': 197,\n",
       " 'leader': 198,\n",
       " 'others': 199,\n",
       " 'attack': 200,\n",
       " 'people': 201,\n",
       " 'another': 202,\n",
       " '6': 203,\n",
       " 'world': 204,\n",
       " 'day': 205,\n",
       " 'car': 206,\n",
       " 'young': 207,\n",
       " 'little': 208,\n",
       " 'california': 209,\n",
       " 'crash': 210,\n",
       " 'last': 211,\n",
       " 'book': 212,\n",
       " 'death': 213,\n",
       " 'gun': 214,\n",
       " 'texas': 215,\n",
       " 'hes': 216,\n",
       " 'sex': 217,\n",
       " 'abuse': 218,\n",
       " 'find': 219,\n",
       " 'truth': 220,\n",
       " 'arizona': 221,\n",
       " 'american': 222,\n",
       " 'pompeo': 223,\n",
       " 'change': 224,\n",
       " 'city': 225,\n",
       " 'kim': 226,\n",
       " 'so': 227,\n",
       " 'music': 228,\n",
       " 'risks': 229,\n",
       " 'being': 230,\n",
       " 'billions': 231,\n",
       " '5': 232,\n",
       " 'family': 233,\n",
       " 'missing': 234,\n",
       " 'gaza': 235,\n",
       " 'vs': 236,\n",
       " 'senate': 237,\n",
       " 'heart': 238,\n",
       " 'justice': 239,\n",
       " 'killing': 240,\n",
       " 'cia': 241,\n",
       " 'come': 242,\n",
       " 'shows': 243,\n",
       " 'turning': 244,\n",
       " 'schools': 245,\n",
       " 'parents': 246,\n",
       " 'law': 247,\n",
       " 'legal': 248,\n",
       " 'economy': 249,\n",
       " 'strike': 250,\n",
       " 'him': 251,\n",
       " 'threat': 252,\n",
       " 'before': 253,\n",
       " 'takes': 254,\n",
       " 'night': 255,\n",
       " 'mueller': 256,\n",
       " 'close': 257,\n",
       " 'cut': 258,\n",
       " 'dream': 259,\n",
       " 'face': 260,\n",
       " 'friends': 261,\n",
       " 'does': 262,\n",
       " 'game': 263,\n",
       " 'dear': 264,\n",
       " 'boss': 265,\n",
       " 'baby': 266,\n",
       " 'kings': 267,\n",
       " 'jail': 268,\n",
       " 'nfl': 269,\n",
       " 'jimmy': 270,\n",
       " 'word': 271,\n",
       " 'hot': 272,\n",
       " 'turns': 273,\n",
       " 'gap': 274,\n",
       " 'got': 275,\n",
       " 'hope': 276,\n",
       " 'making': 277,\n",
       " 'overlooked': 278,\n",
       " 'push': 279,\n",
       " 'high': 280,\n",
       " 'deal': 281,\n",
       " 'heck': 282,\n",
       " 'live': 283,\n",
       " 'families': 284,\n",
       " 'wrong': 285,\n",
       " '2018': 286,\n",
       " 'fix': 287,\n",
       " 'lawyers': 288,\n",
       " 'koreas': 289,\n",
       " 'choose': 290,\n",
       " 'public': 291,\n",
       " 'cuomo': 292,\n",
       " 'steel': 293,\n",
       " 'open': 294,\n",
       " 'scott': 295,\n",
       " 'beyond': 296,\n",
       " 'variety': 297,\n",
       " 'ethics': 298,\n",
       " 'files': 299,\n",
       " 'dept': 300,\n",
       " 'let': 301,\n",
       " 'inside': 302,\n",
       " 'director': 303,\n",
       " 'far': 304,\n",
       " 'side': 305,\n",
       " 'rupauls': 306,\n",
       " 'drag': 307,\n",
       " 'fence': 308,\n",
       " 'sanctions': 309,\n",
       " 'want': 310,\n",
       " 'global': 311,\n",
       " 'great': 312,\n",
       " 'fish': 313,\n",
       " 'market': 314,\n",
       " 'team': 315,\n",
       " 'eat': 316,\n",
       " 'problem': 317,\n",
       " 'miracle': 318,\n",
       " 'tied': 319,\n",
       " 'hours': 320,\n",
       " 'working': 321,\n",
       " 'other': 322,\n",
       " 'yet': 323,\n",
       " 'call': 324,\n",
       " 'allies': 325,\n",
       " 'privacy': 326,\n",
       " 'data': 327,\n",
       " 'experts': 328,\n",
       " 'go': 329,\n",
       " 'met': 330,\n",
       " 'brooklyn': 331,\n",
       " 'didnt': 332,\n",
       " 'russian': 333,\n",
       " 'trial': 334,\n",
       " 'apply': 335,\n",
       " 'college': 336,\n",
       " 'many': 337,\n",
       " '4': 338,\n",
       " 'security': 339,\n",
       " 'year': 340,\n",
       " 'tale': 341,\n",
       " 'social': 342,\n",
       " 'control': 343,\n",
       " 'been': 344,\n",
       " 'hit': 345,\n",
       " 'early': 346,\n",
       " 'behind': 347,\n",
       " 'match': 348,\n",
       " 'ok': 349,\n",
       " 'fears': 350,\n",
       " 'isis': 351,\n",
       " 'walking': 352,\n",
       " 'national': 353,\n",
       " 'presidency': 354,\n",
       " 'student': 355,\n",
       " 'second': 356,\n",
       " 'limits': 357,\n",
       " 'care': 358,\n",
       " 'era': 359,\n",
       " 'south': 360,\n",
       " 'guard': 361,\n",
       " 'rise': 362,\n",
       " 'edge': 363,\n",
       " 'hero': 364,\n",
       " 'tech': 365,\n",
       " 'secret': 366,\n",
       " 'storm': 367,\n",
       " 'gets': 368,\n",
       " 'watch': 369,\n",
       " 'weapons': 370,\n",
       " 'cheerleaders': 371,\n",
       " 'meeting': 372,\n",
       " 'less': 373,\n",
       " 'science': 374,\n",
       " 'dirt': 375,\n",
       " 'times': 376,\n",
       " 'looking': 377,\n",
       " 'win': 378,\n",
       " 'pope': 379,\n",
       " 'stuff': 380,\n",
       " 'wants': 381,\n",
       " 'ruling': 382,\n",
       " 'town': 383,\n",
       " 'cold': 384,\n",
       " 'behavior': 385,\n",
       " 'guns': 386,\n",
       " 'stand': 387,\n",
       " 'human': 388,\n",
       " 'economic': 389,\n",
       " 'tragedy': 390,\n",
       " 'paul': 391,\n",
       " 'them': 392,\n",
       " 'key': 393,\n",
       " 'down': 394,\n",
       " 'given': 395,\n",
       " 'urge': 396,\n",
       " 'kids': 397,\n",
       " 'westworld': 398,\n",
       " 'picture': 399,\n",
       " 'april': 400,\n",
       " '23': 401,\n",
       " 'subway': 402,\n",
       " 'lets': 403,\n",
       " 'impeachment': 404,\n",
       " 'feel': 405,\n",
       " 'told': 406,\n",
       " 'smile': 407,\n",
       " 'meet': 408,\n",
       " 'wild': 409,\n",
       " 'guide': 410,\n",
       " 'business': 411,\n",
       " 'republicans': 412,\n",
       " 'starbucks': 413,\n",
       " 'door': 414,\n",
       " 'heres': 415,\n",
       " 'punch': 416,\n",
       " 'air': 417,\n",
       " 'caution': 418,\n",
       " 'democratic': 419,\n",
       " 'seen': 420,\n",
       " 'west': 421,\n",
       " 'once': 422,\n",
       " 'marriage': 423,\n",
       " 'politics': 424,\n",
       " 'mission': 425,\n",
       " 'support': 426,\n",
       " 'extra': 427,\n",
       " 'stephen': 428,\n",
       " 'colbert': 429,\n",
       " 'doesnt': 430,\n",
       " 'marijuana': 431,\n",
       " 'reading': 432,\n",
       " 'flight': 433,\n",
       " 'navy': 434,\n",
       " 'veteran': 435,\n",
       " 'ban': 436,\n",
       " 'atlanta': 437,\n",
       " 'finally': 438,\n",
       " 'ready': 439,\n",
       " 'todays': 440,\n",
       " 'puzzle': 441,\n",
       " 'deputy': 442,\n",
       " 'gave': 443,\n",
       " 'teenagers': 444,\n",
       " 'training': 445,\n",
       " 'cant': 446,\n",
       " 'remember': 447,\n",
       " '36': 448,\n",
       " 'found': 449,\n",
       " 'look': 450,\n",
       " 'scientists': 451,\n",
       " 'drug': 452,\n",
       " 'start': 453,\n",
       " 'fit': 454,\n",
       " 'syrian': 455,\n",
       " 'israel': 456,\n",
       " 'bush': 457,\n",
       " 'tweets': 458,\n",
       " 'calling': 459,\n",
       " 'genius': 460,\n",
       " 'acrostic': 461,\n",
       " 'strikes': 462,\n",
       " 'americas': 463,\n",
       " 'pregnancy': 464,\n",
       " 'step': 465,\n",
       " 'washington': 466,\n",
       " 'body': 467,\n",
       " 'supreme': 468,\n",
       " 'best': 469,\n",
       " 'run': 470,\n",
       " 'late': 471,\n",
       " 'force': 472,\n",
       " 'france': 473,\n",
       " 'said': 474,\n",
       " 'raid': 475,\n",
       " 'report': 476,\n",
       " 'aide': 477,\n",
       " 'try': 478,\n",
       " 'line': 479,\n",
       " 'loss': 480,\n",
       " 'risk': 481,\n",
       " 'ask': 482,\n",
       " 'trevor': 483,\n",
       " 'noah': 484,\n",
       " 'become': 485,\n",
       " 'affair': 486,\n",
       " 'died': 487,\n",
       " 'travel': 488,\n",
       " 'pollution': 489,\n",
       " 'building': 490,\n",
       " 'details': 491,\n",
       " 'mike': 492,\n",
       " 'immigration': 493,\n",
       " 'much': 494,\n",
       " 'words': 495,\n",
       " 'press': 496,\n",
       " 'energy': 497,\n",
       " 'think': 498,\n",
       " 'kitchen': 499,\n",
       " 'teams': 500,\n",
       " 'roseanne': 501,\n",
       " 'matter': 502,\n",
       " 'warm': 503,\n",
       " 'cohen': 504,\n",
       " 'eye': 505,\n",
       " 'wait': 506,\n",
       " 'seized': 507,\n",
       " 'civil': 508,\n",
       " 'brain': 509,\n",
       " 'die': 510,\n",
       " 'bad': 511,\n",
       " 'island': 512,\n",
       " 'target': 513,\n",
       " 'michael': 514,\n",
       " 'complicated': 515,\n",
       " 'under': 516,\n",
       " 'test': 517,\n",
       " 'did': 518,\n",
       " 'talks': 519,\n",
       " 'put': 520,\n",
       " 'questions': 521,\n",
       " 'dies': 522,\n",
       " 'moves': 523,\n",
       " '15': 524,\n",
       " 'lives': 525,\n",
       " 'james': 526,\n",
       " 'gives': 527,\n",
       " 'warriors': 528,\n",
       " 'mainstream': 529,\n",
       " 'saudi': 530,\n",
       " 'loan': 531,\n",
       " 'puts': 532,\n",
       " 'star': 533,\n",
       " 'broken': 534,\n",
       " 'glass': 535,\n",
       " 'moments': 536,\n",
       " 'sick': 537,\n",
       " 'beat': 538,\n",
       " 'god': 539,\n",
       " 'mean': 540,\n",
       " 'spy': 541,\n",
       " 'immigrants': 542,\n",
       " 'kill': 543,\n",
       " 'shame': 544,\n",
       " 'maybe': 545,\n",
       " 'even': 546,\n",
       " 'land': 547,\n",
       " '14': 548,\n",
       " 'policy': 549,\n",
       " 'agent': 550,\n",
       " 'smart': 551,\n",
       " 'martin': 552,\n",
       " 'luther': 553,\n",
       " 'mind': 554,\n",
       " 'earth': 555,\n",
       " 'action': 556,\n",
       " 'iraq': 557,\n",
       " 'feeling': 558,\n",
       " 'nature': 559,\n",
       " 'church': 560,\n",
       " 'beware': 561,\n",
       " 'away': 562,\n",
       " 'point': 563,\n",
       " 'army': 564,\n",
       " 'va': 565,\n",
       " 'girls': 566,\n",
       " 'play': 567,\n",
       " 'made': 568,\n",
       " 'marathon': 569,\n",
       " 'lies': 570,\n",
       " 'turn': 571,\n",
       " 'fiction': 572,\n",
       " 'terror': 573,\n",
       " 'accept': 574,\n",
       " 'dying': 575,\n",
       " 'victims': 576,\n",
       " 'golden': 577,\n",
       " 'common': 578,\n",
       " 'near': 579,\n",
       " 'cosby': 580,\n",
       " 'revolt': 581,\n",
       " 'offer': 582,\n",
       " 'rule': 583,\n",
       " 'bag': 584,\n",
       " 'tradition': 585,\n",
       " 'failed': 586,\n",
       " 'utah': 587,\n",
       " 'few': 588,\n",
       " 'believe': 589,\n",
       " 'forced': 590,\n",
       " 'artists': 591,\n",
       " 'carter': 592,\n",
       " 'jersey': 593,\n",
       " 'quiz': 594,\n",
       " 'lifethreatening': 595,\n",
       " 'food': 596,\n",
       " 'choosing': 597,\n",
       " 'future': 598,\n",
       " 'gone': 599,\n",
       " 'panel': 600,\n",
       " 'bucks': 601,\n",
       " 'sidewalk': 602,\n",
       " 'van': 603,\n",
       " 'cuts': 604,\n",
       " 'apologies': 605,\n",
       " 'europes': 606,\n",
       " 'iran': 607,\n",
       " 'themselves': 608,\n",
       " 'avengers': 609,\n",
       " 'most': 610,\n",
       " 'movie': 611,\n",
       " 'taking': 612,\n",
       " 'mirror': 613,\n",
       " 'chancellor': 614,\n",
       " 'debate': 615,\n",
       " 'foods': 616,\n",
       " 'citys': 617,\n",
       " 'favorite': 618,\n",
       " 'cruelty': 619,\n",
       " 'robots': 620,\n",
       " 'coffee': 621,\n",
       " 'prince': 622,\n",
       " 'share': 623,\n",
       " 'tolerance': 624,\n",
       " 'stop': 625,\n",
       " 'migrants': 626,\n",
       " 'path': 627,\n",
       " 'aiding': 628,\n",
       " 'europe': 629,\n",
       " 'magic': 630,\n",
       " 'kind': 631,\n",
       " 'losing': 632,\n",
       " 'middle': 633,\n",
       " 'class': 634,\n",
       " 'youll': 635,\n",
       " 'sorry': 636,\n",
       " 'swamp': 637,\n",
       " 'columbia': 638,\n",
       " 'wages': 639,\n",
       " 'painfully': 640,\n",
       " 'wonkish': 641,\n",
       " 'forgotten': 642,\n",
       " 'survival': 643,\n",
       " 'decline': 644,\n",
       " 'mess': 645,\n",
       " 'fake': 646,\n",
       " 'set': 647,\n",
       " 'himself': 648,\n",
       " 'barely': 649,\n",
       " 'quit': 650,\n",
       " '70': 651,\n",
       " 'conspiracy': 652,\n",
       " 'pick': 653,\n",
       " 'wells': 654,\n",
       " 'fargo': 655,\n",
       " 'mayhem': 656,\n",
       " 'fields': 657,\n",
       " '50': 658,\n",
       " 'lack': 659,\n",
       " 'cynthia': 660,\n",
       " 'nixon': 661,\n",
       " 'along': 662,\n",
       " 'promise': 663,\n",
       " 'testing': 664,\n",
       " 'candidates': 665,\n",
       " 'pruitts': 666,\n",
       " 'reforms': 667,\n",
       " 'selling': 668,\n",
       " 'know': 669,\n",
       " 'runners': 670,\n",
       " 'tarot': 671,\n",
       " 'card': 672,\n",
       " 'dress': 673,\n",
       " 'pilot': 674,\n",
       " 'nerves': 675,\n",
       " 'letting': 676,\n",
       " 'al': 677,\n",
       " 'equal': 678,\n",
       " 'returns': 679,\n",
       " 'snake': 680,\n",
       " 'oil': 681,\n",
       " 'betrayed': 682,\n",
       " 'nation': 683,\n",
       " 'fashion': 684,\n",
       " 'lay': 685,\n",
       " 'fine': 686,\n",
       " 'fraud': 687,\n",
       " 'again': 688,\n",
       " 'exfbi': 689,\n",
       " 'sent': 690,\n",
       " 'name': 691,\n",
       " 'sea': 692,\n",
       " 'stands': 693,\n",
       " 'spur': 694,\n",
       " 'means': 695,\n",
       " 'progressive': 696,\n",
       " 'standardized': 697,\n",
       " 'tests': 698,\n",
       " 'antibias': 699,\n",
       " 'goal': 700,\n",
       " 'gentrification': 701,\n",
       " 'shine': 702,\n",
       " 'road': 703,\n",
       " 'deportation': 704,\n",
       " 'voice': 705,\n",
       " 'crazy': 706,\n",
       " 'warming': 707,\n",
       " 'interview': 708,\n",
       " 'launches': 709,\n",
       " 'allout': 710,\n",
       " 'barbara': 711,\n",
       " 'ill': 712,\n",
       " 'treatment': 713,\n",
       " 'blasts': 714,\n",
       " 'slippery': 715,\n",
       " 'around': 716,\n",
       " 'office': 717,\n",
       " 'todo': 718,\n",
       " 'list': 719,\n",
       " 'housing': 720,\n",
       " 'leaves': 721,\n",
       " 'pinch': 722,\n",
       " 'took': 723,\n",
       " 'assad': 724,\n",
       " 'library': 725,\n",
       " 'payment': 726,\n",
       " 'snarl': 727,\n",
       " 'makes': 728,\n",
       " '87': 729,\n",
       " 'comedy': 730,\n",
       " 'rising': 731,\n",
       " 'stars': 732,\n",
       " 'alike': 733,\n",
       " 'coal': 734,\n",
       " 'lobbyist': 735,\n",
       " 'crossword': 736,\n",
       " 'animals': 737,\n",
       " 'whos': 738,\n",
       " 'pentagon': 739,\n",
       " 'signs': 740,\n",
       " 'childhood': 741,\n",
       " 'courts': 742,\n",
       " 'shift': 743,\n",
       " 'saying': 744,\n",
       " 'scandal': 745,\n",
       " 'sticker': 746,\n",
       " 'shock': 747,\n",
       " 'teacher': 748,\n",
       " 'walkouts': 749,\n",
       " 'grip': 750,\n",
       " 'red': 751,\n",
       " 'speaker': 752,\n",
       " 'leave': 753,\n",
       " 'british': 754,\n",
       " 'mexico': 755,\n",
       " 'wish': 756,\n",
       " 'alive': 757,\n",
       " 'longer': 758,\n",
       " 'moscow': 759,\n",
       " 'believes': 760,\n",
       " 'beef': 761,\n",
       " 'stew': 762,\n",
       " 'chinese': 763,\n",
       " 'partner': 764,\n",
       " 'tabloid': 765,\n",
       " 'catches': 766,\n",
       " 'facebooks': 767,\n",
       " 'lot': 768,\n",
       " 'health': 769,\n",
       " 'putin': 770,\n",
       " 'india': 771,\n",
       " 'these': 772,\n",
       " 'begin': 773,\n",
       " 'protest': 774,\n",
       " 'florida': 775,\n",
       " 'colorado': 776,\n",
       " 'thats': 777,\n",
       " 'hopes': 778,\n",
       " 'access': 779,\n",
       " 'hollywood': 780,\n",
       " 'tape': 781,\n",
       " 'focus': 782,\n",
       " 'fbi': 783,\n",
       " 'general': 784,\n",
       " 'leak': 785,\n",
       " 'spring': 786,\n",
       " 'gut': 787,\n",
       " 'advice': 788,\n",
       " 'risotto': 789,\n",
       " 'simple': 790,\n",
       " 'during': 791,\n",
       " 'week': 792,\n",
       " 'ball': 793,\n",
       " 'gluten': 794,\n",
       " 'free': 795,\n",
       " 'seattle': 796,\n",
       " 'exhusband': 797,\n",
       " 'dilemma': 798,\n",
       " 'betting': 799,\n",
       " 'governors': 800,\n",
       " 'collector': 801,\n",
       " 'thriving': 802,\n",
       " 'modern': 803,\n",
       " 'count': 804,\n",
       " 'thousands': 805,\n",
       " 'interest': 806,\n",
       " 'alaska': 807,\n",
       " 'republican': 808,\n",
       " 'midterm': 809,\n",
       " 'elections': 810,\n",
       " 'pose': 811,\n",
       " 'worry': 812,\n",
       " 'view': 813,\n",
       " 'fascism': 814,\n",
       " 'without': 815,\n",
       " 'spending': 816,\n",
       " 'signals': 817,\n",
       " 'boys': 818,\n",
       " 'learning': 819,\n",
       " 'move': 820,\n",
       " 'fast': 821,\n",
       " 'governor': 822,\n",
       " 'those': 823,\n",
       " 'aging': 824,\n",
       " 'warns': 825,\n",
       " 'media': 826,\n",
       " 'exercise': 827,\n",
       " 'allergic': 828,\n",
       " 'signatures': 829,\n",
       " 'military': 830,\n",
       " 'disaster': 831,\n",
       " 'embrace': 832,\n",
       " 'youre': 833,\n",
       " 'defying': 834,\n",
       " 'toll': 835,\n",
       " 'humans': 836,\n",
       " 'store': 837,\n",
       " 'dogs': 838,\n",
       " 'client': 839,\n",
       " 'reports': 840,\n",
       " 'fired': 841,\n",
       " 'coming': 842,\n",
       " 'lavish': 843,\n",
       " 'zone': 844,\n",
       " 'called': 845,\n",
       " 'options': 846,\n",
       " 'paid': 847,\n",
       " 'ice': 848,\n",
       " 'adviser': 849,\n",
       " 'calls': 850,\n",
       " 'polar': 851,\n",
       " 'bears': 852,\n",
       " 'online': 853,\n",
       " 'bust': 854,\n",
       " 'bars': 855,\n",
       " 'private': 856,\n",
       " 'fans': 857,\n",
       " 'remains': 858,\n",
       " 'funny': 859,\n",
       " 'daughter': 860,\n",
       " 'offered': 861,\n",
       " 'massacre': 862,\n",
       " 'prison': 863,\n",
       " 'votes': 864,\n",
       " 'very': 865,\n",
       " 'outsiders': 866,\n",
       " 'gender': 867,\n",
       " 'scrutiny': 868,\n",
       " 'trail': 869,\n",
       " 'nightmare': 870,\n",
       " 'species': 871,\n",
       " 'fuels': 872,\n",
       " 'jacket': 873,\n",
       " 'hair': 874,\n",
       " '81': 875,\n",
       " 'foreign': 876,\n",
       " '9': 877,\n",
       " 'arabian': 878,\n",
       " 'official': 879,\n",
       " 'afghan': 880,\n",
       " 'enters': 881,\n",
       " 'journalism': 882,\n",
       " 'later': 883,\n",
       " 'cry': 884,\n",
       " 'vows': 885,\n",
       " 'price': 886,\n",
       " 'raising': 887,\n",
       " 'song': 888,\n",
       " 'gallery': 889,\n",
       " 'hockey': 890,\n",
       " 'fox': 891,\n",
       " 'fiery': 892,\n",
       " 'chemical': 893,\n",
       " 'hasty': 894,\n",
       " 'viral': 895,\n",
       " 'voters': 896,\n",
       " 'arlee': 897,\n",
       " 'arent': 898,\n",
       " 'friday': 899,\n",
       " 'something': 900,\n",
       " 'every': 901,\n",
       " 'buying': 902,\n",
       " 'ad': 903,\n",
       " 'asparagus': 904,\n",
       " 'blame': 905,\n",
       " 'democracy': 906,\n",
       " 'bracing': 907,\n",
       " 'blood': 908,\n",
       " 'estate': 909,\n",
       " 'jobs': 910,\n",
       " 'raised': 911,\n",
       " 'islam': 912,\n",
       " 'yorks': 913,\n",
       " 'give': 914,\n",
       " 'blast': 915,\n",
       " 'small': 916,\n",
       " 'forces': 917,\n",
       " 'tight': 918,\n",
       " 'labor': 919,\n",
       " 'manhattan': 920,\n",
       " 'intellectual': 921,\n",
       " 'tiny': 922,\n",
       " 'haunting': 923,\n",
       " 'thoughts': 924,\n",
       " 'misconduct': 925,\n",
       " 'usual': 926,\n",
       " 'lisa': 927,\n",
       " 'evil': 928,\n",
       " 'springs': 929,\n",
       " 'stress': 930,\n",
       " 'monkeys': 931,\n",
       " 'gay': 932,\n",
       " 'safety': 933,\n",
       " 'crashes': 934,\n",
       " 'queen': 935,\n",
       " 'indexes': 936,\n",
       " 'indian': 937,\n",
       " 'dreams': 938,\n",
       " 'role': 939,\n",
       " 'macrons': 940,\n",
       " 'locals': 941,\n",
       " 'shot': 942,\n",
       " 'whims': 943,\n",
       " 'sees': 944,\n",
       " 'makers': 945,\n",
       " 'pipe': 946,\n",
       " 'slow': 947,\n",
       " 'quiet': 948,\n",
       " 'ohio': 949,\n",
       " 'primary': 950,\n",
       " 'readers': 951,\n",
       " 'universe': 952,\n",
       " 'older': 953,\n",
       " 'well': 954,\n",
       " 'rents': 955,\n",
       " 'saving': 956,\n",
       " 'worship': 957,\n",
       " 'detail': 958,\n",
       " 'project': 959,\n",
       " 'dissent': 960,\n",
       " 'kennedy': 961,\n",
       " 'assassination': 962,\n",
       " 'edges': 963,\n",
       " 'brazil': 964,\n",
       " 'sports': 965,\n",
       " 'dad': 966,\n",
       " 'amazon': 967,\n",
       " 'trauma': 968,\n",
       " 'anxiety': 969,\n",
       " '100': 970,\n",
       " 'endless': 971,\n",
       " 'video': 972,\n",
       " 'legacy': 973,\n",
       " 'candidate': 974,\n",
       " 'shooting': 975,\n",
       " 'silicon': 976,\n",
       " 'valley': 977,\n",
       " 'light': 978,\n",
       " 'chose': 979,\n",
       " 'heated': 980,\n",
       " 'leads': 981,\n",
       " 'israelis': 982,\n",
       " 'lone': 983,\n",
       " 'journalist': 984,\n",
       " 'sexual': 985,\n",
       " 'assault': 986,\n",
       " 'joke': 987,\n",
       " 'bronx': 988,\n",
       " 'necessary': 989,\n",
       " 'search': 990,\n",
       " 'nights': 991,\n",
       " 'giants': 992,\n",
       " 'streets': 993,\n",
       " 'stocks': 994,\n",
       " 'see': 995,\n",
       " 'teachers': 996,\n",
       " 'spreads': 997,\n",
       " 'african': 998,\n",
       " 'only': 999,\n",
       " 'hard': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[99, 269],\n",
       " [99, 269, 371],\n",
       " [99, 269, 371, 1115],\n",
       " [99, 269, 371, 1115, 582],\n",
       " [99, 269, 371, 1115, 582, 52],\n",
       " [99, 269, 371, 1115, 582, 52, 7],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2, 372],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2, 372, 10],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2, 372, 10, 1116],\n",
       " [100, 3]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "for line in text: # 1,214 개의 샘플에 대해서 샘플을 1개씩 가져온다.\n",
    "    encoded = t.texts_to_sequences([line])[0] # 각 샘플에 대한 정수 인코딩\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "sequences[:11] # 11개의 샘플 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈도수 상위 1번 단어 : the\n",
      "빈도수 상위 582번 단어 : offer\n"
     ]
    }
   ],
   "source": [
    "index_to_word = {}\n",
    "for key, value in t.word_index.items(): # 인덱스를 단어로 바꾸기 위해 index_to_word를 생성\n",
    "    index_to_word[value] = key\n",
    "\n",
    "print('빈도수 상위 1번 단어 :', index_to_word[1])\n",
    "print('빈도수 상위 582번 단어 :', index_to_word[582])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플의 최대 길이 : 24\n"
     ]
    }
   ],
   "source": [
    "max_len=max(len(s) for s in sequences)\n",
    "print('샘플의 최대 길이 :', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0   99  269]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0   99  269  371]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0   99  269  371 1115]]\n"
     ]
    }
   ],
   "source": [
    "# 전체 샘플의 길이를 24(가장 긴 샘플의 길이)로 패딩\n",
    "# 'pre' 옵션을 주면 앞을 0으로 패딩\n",
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
    "print(sequences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    0,   99],\n",
       "       [   0,    0,    0, ...,    0,   99,  269],\n",
       "       [   0,    0,    0, ...,   99,  269,  371],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  170,    8, 3493],\n",
       "       [   0,    0,    0, ...,    8, 3493,  115],\n",
       "       [   0,    0,    0, ..., 3493,  115,    2]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블 데이터 y에 대해서 원-핫 인코딩을 수행\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7803, 23), (7803, 3494))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 설계 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Embedding_Layer (Embedding)  (None, 23, 10)            34940     \n",
      "_________________________________________________________________\n",
      "LSTM_Layer (LSTM)            (None, 128)               71168     \n",
      "_________________________________________________________________\n",
      "Output_Layer (Dense)         (None, 3494)              450726    \n",
      "=================================================================\n",
      "Total params: 556,834\n",
      "Trainable params: 556,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 벡터는 10차원, 은닉 상태 크기는 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_len-1,\n",
    "                    name=\"Embedding_Layer\"))\n",
    "model.add(LSTM(128, name=\"LSTM_Layer\"))\n",
    "model.add(Dense(vocab_size, activation='softmax', name=\"Output_Layer\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "7803/7803 [==============================] - 5s 588us/step - loss: 7.6478 - accuracy: 0.0287\n",
      "Epoch 2/200\n",
      "7803/7803 [==============================] - 4s 555us/step - loss: 7.1093 - accuracy: 0.0306\n",
      "Epoch 3/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 6.9714 - accuracy: 0.0342\n",
      "Epoch 4/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 6.8458 - accuracy: 0.0425\n",
      "Epoch 5/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 6.6948 - accuracy: 0.0477\n",
      "Epoch 6/200\n",
      "7803/7803 [==============================] - 4s 552us/step - loss: 6.5295 - accuracy: 0.0479\n",
      "Epoch 7/200\n",
      "7803/7803 [==============================] - 4s 552us/step - loss: 6.3446 - accuracy: 0.0527\n",
      "Epoch 8/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 6.1521 - accuracy: 0.0579\n",
      "Epoch 9/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 5.9692 - accuracy: 0.0623\n",
      "Epoch 10/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 5.7826 - accuracy: 0.0651\n",
      "Epoch 11/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 5.6090 - accuracy: 0.0706\n",
      "Epoch 12/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 5.4470 - accuracy: 0.0754\n",
      "Epoch 13/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 5.2930 - accuracy: 0.0795\n",
      "Epoch 14/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 5.1474 - accuracy: 0.0868\n",
      "Epoch 15/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 5.0054 - accuracy: 0.0948\n",
      "Epoch 16/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 4.8687 - accuracy: 0.1016\n",
      "Epoch 17/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 4.7367 - accuracy: 0.1174\n",
      "Epoch 18/200\n",
      "7803/7803 [==============================] - 4s 552us/step - loss: 4.6062 - accuracy: 0.1287\n",
      "Epoch 19/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 4.4841 - accuracy: 0.1426\n",
      "Epoch 20/200\n",
      "7803/7803 [==============================] - 4s 552us/step - loss: 4.3649 - accuracy: 0.1522\n",
      "Epoch 21/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 4.2470 - accuracy: 0.1758\n",
      "Epoch 22/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 4.1353 - accuracy: 0.1895\n",
      "Epoch 23/200\n",
      "7803/7803 [==============================] - 4s 555us/step - loss: 4.0262 - accuracy: 0.2065\n",
      "Epoch 24/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 3.9218 - accuracy: 0.2256\n",
      "Epoch 25/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 3.8183 - accuracy: 0.2414\n",
      "Epoch 26/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 3.7181 - accuracy: 0.2613\n",
      "Epoch 27/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 3.6234 - accuracy: 0.2710\n",
      "Epoch 28/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 3.5289 - accuracy: 0.2976\n",
      "Epoch 29/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 3.4385 - accuracy: 0.3089\n",
      "Epoch 30/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 3.3551 - accuracy: 0.3263\n",
      "Epoch 31/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 3.2685 - accuracy: 0.3395\n",
      "Epoch 32/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 3.1892 - accuracy: 0.3565\n",
      "Epoch 33/200\n",
      "7803/7803 [==============================] - 4s 552us/step - loss: 3.1107 - accuracy: 0.3711\n",
      "Epoch 34/200\n",
      "7803/7803 [==============================] - 4s 552us/step - loss: 3.0354 - accuracy: 0.3882\n",
      "Epoch 35/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 2.9628 - accuracy: 0.3966\n",
      "Epoch 36/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 2.8923 - accuracy: 0.4134\n",
      "Epoch 37/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 2.8248 - accuracy: 0.4312\n",
      "Epoch 38/200\n",
      "7803/7803 [==============================] - 4s 552us/step - loss: 2.7565 - accuracy: 0.4405\n",
      "Epoch 39/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 2.6938 - accuracy: 0.4482\n",
      "Epoch 40/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 2.6326 - accuracy: 0.4633\n",
      "Epoch 41/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 2.5716 - accuracy: 0.4740\n",
      "Epoch 42/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 2.5127 - accuracy: 0.4902\n",
      "Epoch 43/200\n",
      "7803/7803 [==============================] - 4s 552us/step - loss: 2.4601 - accuracy: 0.4996\n",
      "Epoch 44/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 2.4036 - accuracy: 0.5085\n",
      "Epoch 45/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 2.3493 - accuracy: 0.5217\n",
      "Epoch 46/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 2.2962 - accuracy: 0.5288\n",
      "Epoch 47/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 2.2499 - accuracy: 0.5406\n",
      "Epoch 48/200\n",
      "7803/7803 [==============================] - 4s 555us/step - loss: 2.2014 - accuracy: 0.5477\n",
      "Epoch 49/200\n",
      "7803/7803 [==============================] - 4s 555us/step - loss: 2.1517 - accuracy: 0.5606\n",
      "Epoch 50/200\n",
      "7803/7803 [==============================] - 4s 552us/step - loss: 2.1075 - accuracy: 0.5703\n",
      "Epoch 51/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 2.0623 - accuracy: 0.5793\n",
      "Epoch 52/200\n",
      "7803/7803 [==============================] - 4s 555us/step - loss: 2.0148 - accuracy: 0.5907\n",
      "Epoch 53/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 1.9767 - accuracy: 0.5962\n",
      "Epoch 54/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 1.9338 - accuracy: 0.6048\n",
      "Epoch 55/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 1.8918 - accuracy: 0.6149\n",
      "Epoch 56/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 1.8514 - accuracy: 0.6228\n",
      "Epoch 57/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 1.8101 - accuracy: 0.6336\n",
      "Epoch 58/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 1.7731 - accuracy: 0.6399\n",
      "Epoch 59/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 1.7367 - accuracy: 0.6458\n",
      "Epoch 60/200\n",
      "7803/7803 [==============================] - 4s 558us/step - loss: 1.6988 - accuracy: 0.6618\n",
      "Epoch 61/200\n",
      "7803/7803 [==============================] - 4s 555us/step - loss: 1.6608 - accuracy: 0.6642\n",
      "Epoch 62/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 1.6285 - accuracy: 0.6709\n",
      "Epoch 63/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 1.5961 - accuracy: 0.6790\n",
      "Epoch 64/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 1.5603 - accuracy: 0.6856\n",
      "Epoch 65/200\n",
      "7803/7803 [==============================] - 4s 569us/step - loss: 1.5286 - accuracy: 0.6885\n",
      "Epoch 66/200\n",
      "7803/7803 [==============================] - 4s 565us/step - loss: 1.4980 - accuracy: 0.6968\n",
      "Epoch 67/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 1.4638 - accuracy: 0.7054\n",
      "Epoch 68/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 1.4317 - accuracy: 0.7140\n",
      "Epoch 69/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 1.4013 - accuracy: 0.7199\n",
      "Epoch 70/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 1.3736 - accuracy: 0.7256\n",
      "Epoch 71/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 1.3436 - accuracy: 0.7325\n",
      "Epoch 72/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 1.3197 - accuracy: 0.73340s - loss:\n",
      "Epoch 73/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 1.2887 - accuracy: 0.7439\n",
      "Epoch 74/200\n",
      "7803/7803 [==============================] - 4s 558us/step - loss: 1.2598 - accuracy: 0.7492\n",
      "Epoch 75/200\n",
      "7803/7803 [==============================] - 4s 559us/step - loss: 1.2339 - accuracy: 0.7542\n",
      "Epoch 76/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 1.2085 - accuracy: 0.75840s - loss: 1.1883 - \n",
      "Epoch 77/200\n",
      "7803/7803 [==============================] - 4s 559us/step - loss: 1.1825 - accuracy: 0.7657\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7803/7803 [==============================] - 4s 561us/step - loss: 1.1581 - accuracy: 0.7671\n",
      "Epoch 79/200\n",
      "7803/7803 [==============================] - 4s 558us/step - loss: 1.1353 - accuracy: 0.7753\n",
      "Epoch 80/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 1.1091 - accuracy: 0.7762\n",
      "Epoch 81/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 1.0875 - accuracy: 0.7826\n",
      "Epoch 82/200\n",
      "7803/7803 [==============================] - 4s 551us/step - loss: 1.0653 - accuracy: 0.7865\n",
      "Epoch 83/200\n",
      "7803/7803 [==============================] - 4s 555us/step - loss: 1.0482 - accuracy: 0.7944\n",
      "Epoch 84/200\n",
      "7803/7803 [==============================] - 4s 550us/step - loss: 1.0241 - accuracy: 0.7979\n",
      "Epoch 85/200\n",
      "7803/7803 [==============================] - 4s 550us/step - loss: 1.0016 - accuracy: 0.8025\n",
      "Epoch 86/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 0.9809 - accuracy: 0.8037\n",
      "Epoch 87/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 0.9614 - accuracy: 0.8093\n",
      "Epoch 88/200\n",
      "7803/7803 [==============================] - 4s 549us/step - loss: 0.9441 - accuracy: 0.8126\n",
      "Epoch 89/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 0.9186 - accuracy: 0.8153\n",
      "Epoch 90/200\n",
      "7803/7803 [==============================] - 4s 551us/step - loss: 0.8990 - accuracy: 0.8194\n",
      "Epoch 91/200\n",
      "7803/7803 [==============================] - 4s 552us/step - loss: 0.8816 - accuracy: 0.8233\n",
      "Epoch 92/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 0.8624 - accuracy: 0.8258\n",
      "Epoch 93/200\n",
      "7803/7803 [==============================] - 4s 555us/step - loss: 0.8452 - accuracy: 0.8321\n",
      "Epoch 94/200\n",
      "7803/7803 [==============================] - 4s 552us/step - loss: 0.8297 - accuracy: 0.8354\n",
      "Epoch 95/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 0.8114 - accuracy: 0.8367\n",
      "Epoch 96/200\n",
      "7803/7803 [==============================] - 4s 550us/step - loss: 0.7942 - accuracy: 0.8398\n",
      "Epoch 97/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 0.7803 - accuracy: 0.8438\n",
      "Epoch 98/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 0.7649 - accuracy: 0.8460\n",
      "Epoch 99/200\n",
      "7803/7803 [==============================] - 4s 551us/step - loss: 0.7481 - accuracy: 0.8483\n",
      "Epoch 100/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 0.7334 - accuracy: 0.8536\n",
      "Epoch 101/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 0.7204 - accuracy: 0.8562\n",
      "Epoch 102/200\n",
      "7803/7803 [==============================] - 4s 560us/step - loss: 0.7035 - accuracy: 0.8604\n",
      "Epoch 103/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 0.6958 - accuracy: 0.8613\n",
      "Epoch 104/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 0.6771 - accuracy: 0.8634\n",
      "Epoch 105/200\n",
      "7803/7803 [==============================] - 4s 550us/step - loss: 0.6628 - accuracy: 0.8639\n",
      "Epoch 106/200\n",
      "7803/7803 [==============================] - 4s 558us/step - loss: 0.6518 - accuracy: 0.8685\n",
      "Epoch 107/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 0.6380 - accuracy: 0.8675\n",
      "Epoch 108/200\n",
      "7803/7803 [==============================] - 4s 555us/step - loss: 0.6256 - accuracy: 0.8709\n",
      "Epoch 109/200\n",
      "7803/7803 [==============================] - 4s 552us/step - loss: 0.6144 - accuracy: 0.8748\n",
      "Epoch 110/200\n",
      "7803/7803 [==============================] - 4s 552us/step - loss: 0.6012 - accuracy: 0.8766\n",
      "Epoch 111/200\n",
      "7803/7803 [==============================] - 4s 555us/step - loss: 0.5900 - accuracy: 0.8783\n",
      "Epoch 112/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 0.5804 - accuracy: 0.8800\n",
      "Epoch 113/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 0.5696 - accuracy: 0.8817\n",
      "Epoch 114/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 0.5601 - accuracy: 0.8822\n",
      "Epoch 115/200\n",
      "7803/7803 [==============================] - 4s 552us/step - loss: 0.5487 - accuracy: 0.8848\n",
      "Epoch 116/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 0.5412 - accuracy: 0.8862\n",
      "Epoch 117/200\n",
      "7803/7803 [==============================] - 4s 550us/step - loss: 0.5343 - accuracy: 0.8880\n",
      "Epoch 118/200\n",
      "7803/7803 [==============================] - 4s 555us/step - loss: 0.5242 - accuracy: 0.8908\n",
      "Epoch 119/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 0.5495 - accuracy: 0.8845\n",
      "Epoch 120/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 0.5128 - accuracy: 0.8926\n",
      "Epoch 121/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 0.4951 - accuracy: 0.8971\n",
      "Epoch 122/200\n",
      "7803/7803 [==============================] - 4s 560us/step - loss: 0.4842 - accuracy: 0.8953\n",
      "Epoch 123/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 0.4767 - accuracy: 0.8981\n",
      "Epoch 124/200\n",
      "7803/7803 [==============================] - 4s 559us/step - loss: 0.4684 - accuracy: 0.8993\n",
      "Epoch 125/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 0.4617 - accuracy: 0.9000\n",
      "Epoch 126/200\n",
      "7803/7803 [==============================] - 4s 554us/step - loss: 0.4534 - accuracy: 0.9030\n",
      "Epoch 127/200\n",
      "7803/7803 [==============================] - 4s 559us/step - loss: 0.4492 - accuracy: 0.9041\n",
      "Epoch 128/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 0.4414 - accuracy: 0.9047\n",
      "Epoch 129/200\n",
      "7803/7803 [==============================] - 4s 559us/step - loss: 0.4335 - accuracy: 0.9052\n",
      "Epoch 130/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 0.4467 - accuracy: 0.9027\n",
      "Epoch 131/200\n",
      "7803/7803 [==============================] - 4s 558us/step - loss: 0.4351 - accuracy: 0.9041\n",
      "Epoch 132/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 0.4162 - accuracy: 0.9062\n",
      "Epoch 133/200\n",
      "7803/7803 [==============================] - 4s 561us/step - loss: 0.4086 - accuracy: 0.9063\n",
      "Epoch 134/200\n",
      "7803/7803 [==============================] - 4s 558us/step - loss: 0.4039 - accuracy: 0.9077\n",
      "Epoch 135/200\n",
      "7803/7803 [==============================] - 4s 560us/step - loss: 0.3980 - accuracy: 0.9091\n",
      "Epoch 136/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 0.3929 - accuracy: 0.9103\n",
      "Epoch 137/200\n",
      "7803/7803 [==============================] - 4s 560us/step - loss: 0.3881 - accuracy: 0.9088\n",
      "Epoch 138/200\n",
      "7803/7803 [==============================] - 4s 563us/step - loss: 0.3822 - accuracy: 0.9097\n",
      "Epoch 139/200\n",
      "7803/7803 [==============================] - 4s 560us/step - loss: 0.3769 - accuracy: 0.9125\n",
      "Epoch 140/200\n",
      "7803/7803 [==============================] - 4s 559us/step - loss: 0.3714 - accuracy: 0.9131\n",
      "Epoch 141/200\n",
      "7803/7803 [==============================] - 4s 561us/step - loss: 0.3677 - accuracy: 0.9141\n",
      "Epoch 142/200\n",
      "7803/7803 [==============================] - 5s 606us/step - loss: 0.3632 - accuracy: 0.9150\n",
      "Epoch 143/200\n",
      "7803/7803 [==============================] - 4s 567us/step - loss: 0.3622 - accuracy: 0.9129\n",
      "Epoch 144/200\n",
      "7803/7803 [==============================] - 4s 558us/step - loss: 0.3798 - accuracy: 0.9120\n",
      "Epoch 145/200\n",
      "7803/7803 [==============================] - 4s 562us/step - loss: 0.3740 - accuracy: 0.9099\n",
      "Epoch 146/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 0.3601 - accuracy: 0.9140\n",
      "Epoch 147/200\n",
      "7803/7803 [==============================] - 4s 565us/step - loss: 0.3495 - accuracy: 0.9134\n",
      "Epoch 148/200\n",
      "7803/7803 [==============================] - 4s 558us/step - loss: 0.3429 - accuracy: 0.9140\n",
      "Epoch 149/200\n",
      "7803/7803 [==============================] - 4s 560us/step - loss: 0.3378 - accuracy: 0.9150\n",
      "Epoch 150/200\n",
      "7803/7803 [==============================] - 4s 562us/step - loss: 0.3363 - accuracy: 0.9171\n",
      "Epoch 151/200\n",
      "7803/7803 [==============================] - 4s 560us/step - loss: 0.3347 - accuracy: 0.9139\n",
      "Epoch 152/200\n",
      "7803/7803 [==============================] - 4s 561us/step - loss: 0.3280 - accuracy: 0.9148\n",
      "Epoch 153/200\n",
      "7803/7803 [==============================] - 4s 559us/step - loss: 0.3250 - accuracy: 0.9155\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7803/7803 [==============================] - 4s 558us/step - loss: 0.3226 - accuracy: 0.9167\n",
      "Epoch 155/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 0.3205 - accuracy: 0.9152\n",
      "Epoch 156/200\n",
      "7803/7803 [==============================] - 4s 558us/step - loss: 0.3178 - accuracy: 0.9153\n",
      "Epoch 157/200\n",
      "7803/7803 [==============================] - 4s 560us/step - loss: 0.3154 - accuracy: 0.9162\n",
      "Epoch 158/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 0.3146 - accuracy: 0.9163\n",
      "Epoch 159/200\n",
      "7803/7803 [==============================] - 4s 558us/step - loss: 0.3110 - accuracy: 0.9182\n",
      "Epoch 160/200\n",
      "7803/7803 [==============================] - 4s 558us/step - loss: 0.3075 - accuracy: 0.9177\n",
      "Epoch 161/200\n",
      "7803/7803 [==============================] - 4s 560us/step - loss: 0.3081 - accuracy: 0.9171\n",
      "Epoch 162/200\n",
      "7803/7803 [==============================] - 4s 558us/step - loss: 0.3066 - accuracy: 0.9148\n",
      "Epoch 163/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 0.3024 - accuracy: 0.9170\n",
      "Epoch 164/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 0.3002 - accuracy: 0.9176\n",
      "Epoch 165/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 0.3026 - accuracy: 0.9159\n",
      "Epoch 166/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 0.3010 - accuracy: 0.9172\n",
      "Epoch 167/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 0.3071 - accuracy: 0.9164\n",
      "Epoch 168/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 0.3069 - accuracy: 0.9154\n",
      "Epoch 169/200\n",
      "7803/7803 [==============================] - 4s 553us/step - loss: 0.2944 - accuracy: 0.9171\n",
      "Epoch 170/200\n",
      "7803/7803 [==============================] - 4s 558us/step - loss: 0.2901 - accuracy: 0.9182\n",
      "Epoch 171/200\n",
      "7803/7803 [==============================] - 4s 562us/step - loss: 0.2888 - accuracy: 0.9162\n",
      "Epoch 172/200\n",
      "7803/7803 [==============================] - 4s 556us/step - loss: 0.2877 - accuracy: 0.9163\n",
      "Epoch 173/200\n",
      "7803/7803 [==============================] - 4s 562us/step - loss: 0.2870 - accuracy: 0.9164\n",
      "Epoch 174/200\n",
      "7803/7803 [==============================] - 4s 560us/step - loss: 0.2847 - accuracy: 0.9171\n",
      "Epoch 175/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 0.2844 - accuracy: 0.9177\n",
      "Epoch 176/200\n",
      "7803/7803 [==============================] - 4s 561us/step - loss: 0.2836 - accuracy: 0.9180\n",
      "Epoch 177/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 0.2813 - accuracy: 0.9170\n",
      "Epoch 178/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 0.2820 - accuracy: 0.9167\n",
      "Epoch 179/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 0.2829 - accuracy: 0.9161\n",
      "Epoch 180/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 0.2812 - accuracy: 0.9158\n",
      "Epoch 181/200\n",
      "7803/7803 [==============================] - 4s 559us/step - loss: 0.3169 - accuracy: 0.9099\n",
      "Epoch 182/200\n",
      "7803/7803 [==============================] - 4s 555us/step - loss: 0.3009 - accuracy: 0.9140\n",
      "Epoch 183/200\n",
      "7803/7803 [==============================] - 4s 562us/step - loss: 0.2791 - accuracy: 0.9159\n",
      "Epoch 184/200\n",
      "7803/7803 [==============================] - 4s 561us/step - loss: 0.2761 - accuracy: 0.9155\n",
      "Epoch 185/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 0.2736 - accuracy: 0.9175\n",
      "Epoch 186/200\n",
      "7803/7803 [==============================] - 4s 571us/step - loss: 0.2734 - accuracy: 0.9171\n",
      "Epoch 187/200\n",
      "7803/7803 [==============================] - 4s 567us/step - loss: 0.2726 - accuracy: 0.9158\n",
      "Epoch 188/200\n",
      "7803/7803 [==============================] - 4s 568us/step - loss: 0.2728 - accuracy: 0.9166\n",
      "Epoch 189/200\n",
      "7803/7803 [==============================] - 5s 577us/step - loss: 0.2719 - accuracy: 0.9168\n",
      "Epoch 190/200\n",
      "7803/7803 [==============================] - 4s 568us/step - loss: 0.2710 - accuracy: 0.9164\n",
      "Epoch 191/200\n",
      "7803/7803 [==============================] - 4s 568us/step - loss: 0.2709 - accuracy: 0.9150\n",
      "Epoch 192/200\n",
      "7803/7803 [==============================] - 4s 571us/step - loss: 0.2710 - accuracy: 0.9163\n",
      "Epoch 193/200\n",
      "7803/7803 [==============================] - 4s 572us/step - loss: 0.2701 - accuracy: 0.9167\n",
      "Epoch 194/200\n",
      "7803/7803 [==============================] - 4s 558us/step - loss: 0.2704 - accuracy: 0.9163\n",
      "Epoch 195/200\n",
      "7803/7803 [==============================] - 4s 562us/step - loss: 0.2686 - accuracy: 0.9173\n",
      "Epoch 196/200\n",
      "7803/7803 [==============================] - 4s 561us/step - loss: 0.2690 - accuracy: 0.9175\n",
      "Epoch 197/200\n",
      "7803/7803 [==============================] - 4s 557us/step - loss: 0.2839 - accuracy: 0.9144\n",
      "Epoch 198/200\n",
      "7803/7803 [==============================] - 4s 562us/step - loss: 0.2835 - accuracy: 0.9149\n",
      "Epoch 199/200\n",
      "7803/7803 [==============================] - 4s 558us/step - loss: 0.2737 - accuracy: 0.9159\n",
      "Epoch 200/200\n",
      "7803/7803 [==============================] - 4s 562us/step - loss: 0.2683 - accuracy: 0.91590s - loss: 0.258\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n",
    "    sentence = ''\n",
    "    for _ in range(n): # n번 반복\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=23, padding='pre') # 데이터에 대한 패딩\n",
    "        result = model.predict_classes(encoded, verbose=0)\n",
    "          # 입력한 X(현재 단어)에 대해서 y를 예측하고 y(예측한 단어)를 result에 저장.\n",
    "        for word, index in t.word_index.items(): \n",
    "            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
    "                break # 해당 단어가 예측 단어이므로 break\n",
    "        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장\n",
    "\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i disapprove of school vouchers can i still apply for them\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, 'i', 10))\n",
    "# 임의의 단어 'i'에 대해서 10개의 단어를 추가 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to make a crossword puzzle cuomo and a bear nightmare\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, 'how', 10))\n",
    "# 임의의 단어 'how'에 대해서 10개의 단어를 추가 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "former fbi deputy director is faulted in scathing inspector general report\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, 'former', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
